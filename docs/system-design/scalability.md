# Scalability: Scaling, Partitioning, Sharding, Caching, Data Locality, Load Balancing

_Как справляться с увеличением нагрузки?_

Сервис является масштабируемым, если его производительность увеличивается пропорционально добавляемым ресурсам. Как правило, увеличение производительности означает обслуживание большего количества единиц работы, но это может быть и обработка более крупных единиц работы, например, при росте массивов данных.

Другой способ взглянуть на производительность в сравнении с масштабируемостью:
- Если у вас проблемы с производительностью, то ваша система работает медленно для одного пользователя.
- Если у вас проблемы с масштабируемостью, то ваша система работает быстро для одного пользователя, но медленно при большой нагрузке.

---

**Latency (Латентность)** — это время выполнения некоторого действия или получения некоторого результата.

**Throughput (Пропускная способность)** — это количество таких действий или результатов в единицу времени.

Как правило, вы должны стремиться к максимальной пропускной способности при приемлемой задержке.

---

## Scaling

###  Варианты масштабирования

#### Vertical scaling

Увеличение мощности одного сервера.
    - Добавляем больше CPU.
    - Ставим более быстрый диск.

_Плюсы:_
    - Проще в реализации: не нужно изменять код.
    - Нет проблем с синхронизацией данных между машинами.
_Минусы:_
    - Аппаратный предел (нельзя бесконечно наращивать).
    - Более мощное «железо» — дороже.
    - Если сервер упал — всё сломалось.

#### Horizontal scaling

Добавление новых серверов в систему.
    - Запускаем несколько инстансов приложения на разных серверах.
    - Используем балансировщик нагрузки (load balancer).
    - В кластерах БД — добавляем новые узлы.
  
_Плюсы:_
- Почти неограниченное масштабирование.
  - Повышенная отказоустойчивость.
  - Лучше подходит для облачных решений и микросервисной архитектуры.

_Минусы:_
  - Сложнее реализовать (сессии, синхронизация, распределённые транзакции).
  - Требует дополнительных решений: балансировка, распределённое хранилище и т.д.


---

## Partitioning

---

## Sharding

Гризонтальное разбиение данных на независимые подмножества (шарды), которые хранятся на разных узлах.

### Цель
- Масштабирование по записи (в отличие от репликации, которая масштабирует чтения).
- Устранение "горячих" узлов.
- Распределение нагрузки.

### Стратегии

#### Hash-based Sharding
- Вычисляется хэш по ключу (например, user_id) и берётся модуль от количества шардов.
- _Пример:_ shard_id = hash(user_id) % N
- Равномерное распределение.
- Переход на другой N = перераспределение всего хэш-пространства (можно решить через consistent hashing).
#### Range-based Sharding
- Каждый шард обслуживает определённый диапазон значений.
- _Пример:_ Шард A — ID 1–1000, Шард B — 1001–2000.
- Простота.
- Возможна неравномерная нагрузка (hotspot), сложнее масштабировать.
#### Directory-based Sharding (Lookup Table)
- Существует отдельная таблица/сервис, указывающая, где хранятся данные.
- Гибкость в перемещении данных.
- Bottleneck в lookup-сервисе, необходимость в кэшировании.

### Сложности
- Cross-shard queries: агрегаты, joins и сортировки — затруднены.
- _Resharding:_ требует перемещения данных между узлами без потери доступности.
- Transaction Management: затруднена реализация распределённых транзакций (см. 2PC, saga pattern).
- Shard-aware application logic: клиент должен знать, куда отправить запрос.

### Consistent Hashing

_Проблема:_ Обычный hash-based sharding (hash(key) % N) при изменении N (кол-ва шардов) приводит к перераспределению всех ключей, что = перегрузка + кеш-инвалидность + потери доступности.
_Решение:_ Consistent Hashing — кольцевая структура, 

_где:_
- каждый ключ и нода хэшируются в одно адресное пространство (0…2³²).
- ключ идёт на ближайшую по часовой стрелке ноду.
- при добавлении/удалении ноды перераспределяется лишь часть ключей (~1/N).

_Применение:_
- DynamoDB, Cassandra, Riak, Redis Cluster, Envoy.
- Устойчиво к масштабированию/отказу узлов.

_Особенности:_
- Использует виртуальные ноды (virtual nodes) — каждая физическая нода может быть представлена несколькими логическими узлами на кольце для равномерности.
- Требует hash-функции с хорошим распределением (например, MurmurHash).

### Auto-sharding и Dynamic Resharding

#### Auto-sharding
Система автоматически определяет, как разделять данные и как перемещать их между шардами.
   
_Примеры:_
- MongoDB (shard key + config server).
- Vitess (MySQL масштабирование с автоматическим шардингом и query routing).
- Google Spanner — динамический шардинг на уровне SSTable, автоматическое разбиение по latency hot-spot.

#### Dynamic Resharding
Перераспределение данных между шардами во время работы. Может выполняться вручную (например, перезапуск shard split) или автоматически (например, при hotspot detection).
  
_Технические трудности:_
- Миграция данных между узлами без простоев.
- Дедупликация, если в процессе миграции приходят записи в оба шарда.
- Непрерывность обслуживания: нужно решать через dual writes, versioning, gossip-протоколы.

### Shard Rebalancing
   
_Цель:_ Равномерно распределить данные и нагрузку между шардами, особенно после добавления/удаления узлов.
   
_Решения:_
- Rebalancer отслеживает метрики (например, размер партиции, QPS, latency) и триггерит миграции.
- Уровень данных (файлы, документы, range).
- Уровень логики (объекты, ключи, записи).
  
_Примеры:_
- Kafka — ребалансирует партиции при изменении числа брокеров/консьюмеров.
- Elasticsearch — перемещает shard replicas и primaries по нодам.
- Cassandra — автоматическое перемещение токенов при добавлении узлов.
  
_Трудности:_
- Load spikes во время миграции.
- Перекрестная нагрузка на сеть и диск.
- Необходимость throttling миграций.

---

## Caching

---

## Data Locality

---

## Load Balancer

Распределение входящих запросов между множеством серверов.

### Методы
- Round-Robin (по очереди).
- По нагрузке (отправлять на менее загруженный).
- Sticky Sessions (отправлять пользователя всегда на один и тот же сервер).

### Варианты реализации
- DNS (через A-записи с разными IP).
- HAProxy, LVS – софтварные решения.
- Аппаратные балансировщики (F5, Citrix) — дорогие.

### Взаимодействие с DNS
Обычно клиент взаимодействует с балансировщиком по доменному имени.
- Клиент выполняет DNS-запрос A или AAAA для имени api.example.com.
- DNS возвращает один или несколько IP-адресов — обычно это IP балансировщика (или прокси-сервиса, например Cloudflare).
- Клиент инициирует соединение по IP балансировщика.
- Балансировщик сам определяет, куда отправить запрос — на какой backend.
- DNS не знает о реальных бэкендах и не участвует в балансировке на уровне конкретных серверов, если используется стандартный подход.

---

### Балансировщики (по модели OSI)
1. L3
    - Редко используется как отдельная категория, но может иметь смысл, особенно при работе с маршрутизаторами.
    - Маршрутизация трафика по IP без учёта портов/протоколов.

2. L4 — Network Load Balancer. Быстрая и простая балансировка по IP/портам (TCP/UDP).
    - Оперирует TCP/UDP заголовками.
    - Решения принимаются на основе IP-адресов источника/получателя и портов.
    - Не "знает" содержимого пакета (нет доступа к HTTP-заголовкам или телу запроса).
    - Очень быстрый, минимальный overhead.
    - Используется для low-latency и high-throughput приложений (например, VoIP, гейминг, MQTT, TLS passthrough).

3. L5-L6
    - Контролируют TLS session, SSL handshake, SNI.
    - Поддерживают sticky sessions по TLS session ID.
    - Важны для HTTPS проксирования без расшифровки.

4. L7 — Application Load Balancer. Умная маршрутизация по контенту (HTTP, HTTPS).
    - Работает на уровне приложений (Layer 7).
    - Понимает HTTP, HTTPS, WebSocket, gRPC, SMTP, FTP протоколы.
    - Маршрутизацию по Host, Path, Cookie, Header.
    - Может выполнять SSL/TLS termination (раскрытие TLS-трафика на себе).
    - Позволяет content-based routing (напр., один URL к одному сервису, другой — к другому).
    - Rate Limiting, Caching, WAF (в L7-proxy).
    - Имеет больше логики, следовательно — выше задержки, но даёт гибкость.

5. DNS
    - Уровень L3/L7 (через DNS).

6. Global
    - Уровень L4/L7 (с Anycast IP или TCP proxy).

### Use case
- L4: распределение трафика по микросервисам без разборки протокола.
- L7: гибкое управление трафиком между сервисами API Gateway-style.

### Недостатоки балансировщика нагрузки
- Балансировщик нагрузки может стать узким местом в производительности, если у него недостаточно ресурсов или если он неправильно настроен.
- Внедрение балансировщика нагрузки для устранения единой точки отказа приводит к увеличению сложности.
- Один балансировщик нагрузки - это единственная точка отказа, а настройка нескольких балансировщиков нагрузки еще больше увеличивает сложность.

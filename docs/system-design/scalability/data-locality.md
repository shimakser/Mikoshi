# Data Locality

**Локальность данных** — это свойство системы, при котором данные и вычисления над ними находятся “близко” друг к другу, то есть операции над данными выполняются с минимальными задержками, связанными с их передачей по сети или между уровнями памяти.
В контексте распределённых систем это означает — _минимизацию удалённых вызовов, сетевых хопов и кросс-датацентровых запросов_.

---

## Виды локальности

### Spatial locality (пространственная локальность)

Если данные A и B часто запрашиваются вместе — их стоит хранить рядом (в одном блоке, на одном узле, в одном shard).

_Примеры:_
- Сохранять пост и комментарии к нему на одном сервере.
- В базе данных хранить записи пользователя и связанные сущности (профиль, настройки, права) на одном шард-сегменте.

### Temporal locality (временная локальность)

Если данные недавно использовались, вероятно, они будут востребованы снова.
→ Это используется в **кэшах, буферах, OS page cache, CPU cache**.

_Пример:_ недавние результаты поискового запроса, часто вызываемые профили пользователей, страницы из "hot set" данных.

### Compute locality (локальность вычислений)

Вычисления перемещаются туда, где хранятся данные (data gravity), а не наоборот.
Это ключ к эффективности в системах Big Data, ML pipelines, serverless analytics.

---

## Почему это важно

- **Latency reduction** — минимизация сетевых RTT при чтении данных.
- **Throughput growth** — меньше межнодовых запросов = меньше contention на сетевых интерфейсах.
- **Cost efficiency** — снижение расходов на egress traffic между зонами или регионами.
- **Failure isolation** — локализация отказов, меньшая связанность = меньше cascading failures.
- **Consistency performance tradeoff** — меньше межрегиональных round-trips → меньше консистентностных задержек.

---

## Data Locality в распределённых системах

### Geo-locality (географическая локальность)

Размещение данных ближе к пользователям или к точкам обработки трафика.

_Примеры:_
- CDN (Content Delivery Network): реплицирует статический контент в edge-локации.
- Geo-partitioned databases (например, Spanner, CockroachDB): данные физически находятся в регионе пользователя.
- AWS Local Zones / Azure Edge Zones: минимизация latency до вычислений.

_Компромисс:_ межрегиональные транзакции → высокая задержка, конфликт интересов между latency и consistency.

### Topology-aware placement

Учет физической и логической топологии при распределении данных: rack, datacenter, zone, region.

_Пример:_
HDFS при репликации сохраняет одну копию на локальной ноде, одну в пределах rack, одну на другом rack.
→ устойчивость к отказам rack-level и locality-aware чтение (чтение локальной копии при возможности).

### Locality-aware schedulers

Системы, которые принимают решения о запуске задач исходя из того, где находятся данные:
- YARN, Mesos, Kubernetes, Flink — имеют планировщики, умеющие учитывать locality.
- Data-aware operators: Spark RDD/DF API может выполнять coalesce или repartition с учётом locality.

---

## Стратегии повышения локальности

### Data partitioning по access pattern

Если приложение читает данные в определённом порядке или группах, шарды нужно формировать по этим access patterns, а не по случайным ключам.

_Пример:_
Если пользователи обычно работают с заказами только своей компании → шардинг по company_id, а не по order_id.

### Colocation (совместное размещение данных)

Хранение взаимосвязанных сущностей в одном шарде или узле.

_Пример:_
В соцсети — пост, лайки и комментарии в одном шарде.
В финансовой системе — счёт и транзакции по нему в одной партиции.

_Trade-off:_ высокая связность данных может привести к hotspot-шардам. Нужно балансировать между локальностью и равномерностью нагрузки.

### Data affinity routing

Направление запросов к узлу, где уже есть кеш или данные.
Используется, например, в distributed caches (Hazelcast, Ignite) или actor-системах (Akka Cluster, Orleans).

_Пример:_
Actor UserSessionActor живёт на конкретной ноде; все запросы к пользователю идут туда, обеспечивая кэшированную, локальную работу без сетевых hops.

### Replication-aware query planning

Если данные реплицированы, запросы можно направлять к ближайшей реплике (read-local replica).

_Пример:_
Cassandra и ScyllaDB имеют стратегию LOCAL_QUORUM, при которой запрос читается только с узлов своего региона.

---

## Data Locality и кэширование

Кэширование — это частный случай temporal locality, но на системном уровне взаимодействует и с spatial locality.

Хорошие стратегии кэширования учитывают:
- где хранятся данные (edge cache, near-cache, in-process cache);
- какие данные чаще запрашиваются вместе;
- можно ли реплицировать “горячие” блоки ближе к CPU или пользователю.

В системах вроде Redis Cluster или CDN — локальность = основа производительности.

---

## Data Gravity и перенос вычислений к данным

Data Gravity — принцип, согласно которому данные тяжело переносимы, а приложения и вычисления должны "прилипать" к ним.
При больших объёмах данных переносить вычисления дешевле, чем гонять терабайты по сети.

_Реализации:_
- Spark, Flink — выполняют операции map/reduce на нодах, где находятся данные.
- Lambda architectures — batch/stream вычисления выполняются как можно ближе к source.
- ML pipelines (TFX, Vertex AI) — обучение производится в регионе, где хранятся датасеты.

---

## Метрики локальности

| Метрика                   | Описание                                                     |
| ------------------------- | ------------------------------------------------------------ |
| **Local hit ratio**       | Доля запросов, обслуженных локально (из кеша/шарда/региона). |
| **Cross-node requests %** | Процент запросов, требующих сетевого перехода.               |
| **Average hop count**     | Среднее число узлов, через которые проходит запрос.          |
| **Remote read latency**   | Средняя задержка чтения удалённых данных.                    |
| **Network egress volume** | Объём межнодового или межрегионального трафика.              |

---

## Компромиссы и сложности

| Проблема                    | Пояснение                                                                      |
| --------------------------- | ------------------------------------------------------------------------------ |
| **Hotspots**                | Слишком "локализованные" данные → перегрузка узлов.                            |
| **Consistency vs locality** | Локальные копии ≠ свежие данные; возникает eventual consistency.               |
| **Migration cost**          | Перемещение данных для улучшения locality → дорого по I/O.                     |
| **Scheduling conflicts**    | Задача может требовать нескольких наборов данных, находящихся на разных узлах. |

---

## Примеры реализаций

- **HDFS / Hadoop / Spark**: job placement с учётом block locality.
- **Cassandra**: replica placement strategy (NetworkTopologyStrategy).
- **CockroachDB**: geo-partitioning на уровне таблиц и range'ов.
- **Spanner**: locality hints и placement configs.
- **Snowflake / BigQuery**: data locality abstraction — пользователю не видно, но оптимизатор выполняет locality-aware query planning.
- **Kafka**: data locality при чтении из partition → consumer получает данные из “своего” брокера.

--- 

## Практические рекомендации

- Минимизируй удалённые вызовы: каждый сетевой hop добавляет десятки миллисекунд.
- Храни данные ближе к потребителям: user-centric или region-based partitioning.
- Кэшируй локально: near-cache или in-memory cache при высоком QPS.
- Выноси вычисления к данным: map-reduce, function shipping.
- Измеряй локальность: мониторинг cross-node latency и remote read ratio.
- Проектируй shard layout с учётом query patterns.

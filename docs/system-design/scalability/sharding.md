# Sharding

Это распределение данных по разным физическим узлам (shards), где каждый шард хранит подмножество общего набора данных (обычно по ключу).

Это основа масштабирования по записи и горизонтального распределения нагрузки в системах с большим объёмом данных.

## Цель
- Горизонтальное масштабирование write-нагрузки.
- Локализация данных для уменьшения latency.
- Изоляция отказов (failure isolation).
- Предсказуемое распределение нагрузки между нодами.

---

## Стратегии

### Hash-based Sharding
- Вычисляется хэш по ключу (например, user_id) и берётся модуль от количества шардов.
- _Пример:_ shard_id = hash(user_id) % N
- Равномерное распределение.
- Переход на другой N = перераспределение всего хэш-пространства (можно решить через consistent hashing).

### Range-based Sharding
- Каждый шард обслуживает определённый диапазон значений.
- _Пример:_ Шард A — ID 1–1000, Шард B — 1001–2000.
- Простота.
- Возможна неравномерная нагрузка (hotspot), сложнее масштабировать.

### Directory-based Sharding (Lookup Table)
- Существует отдельная таблица/сервис, указывающая, где хранятся данные.
- Гибкость в перемещении данных.
- Bottleneck в lookup-сервисе, необходимость в кэшировании.

| Тип             | Распределение      | Преимущества            | Недостатки         | Примеры                       |
| --------------- | ------------------ | ----------------------- | ------------------ | ----------------------------- |
| Hash-based      | hash(key) % N      | Равномерная нагрузка    | Сложное resharding | Redis Cluster, Cassandra      |
| Range-based     | Диапазоны значений | Простота, range queries | Hotspots           | HBase, Spanner                |
| Directory-based | Lookup table       | Гибкость, динамика      | Bottleneck lookup  | MongoDB config server, Vitess |

---

## Consistent Hashing

_Проблема:_ Обычный hash-based sharding (hash(key) % N) при изменении N (кол-ва шардов) приводит к перераспределению всех ключей, что = перегрузка + кеш-инвалидность + потери доступности.
_Решение:_ Consistent Hashing — кольцевая структура,

_где:_
- каждый ключ и нода хэшируются в одно адресное пространство (0…2³²).
- ключ идёт на ближайшую по часовой стрелке ноду.
- при добавлении/удалении ноды перераспределяется лишь часть ключей (~1/N).

_Применение:_
- DynamoDB, Cassandra, Riak, Redis Cluster, Envoy.
- Устойчиво к масштабированию/отказу узлов.

_Особенности:_
- Использует виртуальные ноды (virtual nodes) — каждая физическая нода может быть представлена несколькими логическими узлами на кольце для равномерности.
- Требует hash-функции с хорошим распределением (например, MurmurHash).


      0
      +---------+
      /           \
      N4             N1
      |              \
      |               \
      N3---------------N2
      2^32

→ ключи хэшируются в кольцевое пространство; при добавлении/удалении ноды перераспределяется только её диапазон.

---

## Auto-sharding и Dynamic Resharding

### Auto-sharding
Система автоматически определяет, как разделять данные и как перемещать их между шардами.

_Требует:_
- Механизм метаданных (config service, coordinator, Zookeeper/etcd).
- Поддержку "dual writes" во время миграции.
- Background job для data move с throttling.
- Eventual consistency: чтения могут временно видеть старые данные.

_Примеры:_
- MongoDB (shard key + config server).
- Vitess (MySQL масштабирование с автоматическим шардингом и query routing).
- Google Spanner — динамический шардинг на уровне SSTable, автоматическое разбиение по latency hot-spot.

### Dynamic Resharding
Перераспределение данных между шардами во время работы. Может выполняться вручную (например, перезапуск shard split) или автоматически (например, при hotspot detection).

_Технические трудности:_
- Миграция данных между узлами без простоев.
- Дедупликация, если в процессе миграции приходят записи в оба шарда.
- Непрерывность обслуживания: нужно решать через dual writes, versioning, gossip-протоколы.

## Shard Rebalancing

_Цель:_ Равномерно распределить данные и нагрузку между шардами, особенно после добавления/удаления узлов.

_Решения:_
- Rebalancer отслеживает метрики (например, размер партиции, QPS, latency) и триггерит миграции.
- Уровень данных (файлы, документы, range).
- Уровень логики (объекты, ключи, записи).

_Примеры:_
- Kafka — ребалансирует партиции при изменении числа брокеров/консьюмеров.
- Elasticsearch — перемещает shard replicas и primaries по нодам.
- Cassandra — автоматическое перемещение токенов при добавлении узлов.

_Трудности:_
- Load spikes во время миграции.
- Перекрестная нагрузка на сеть и диск.
- Необходимость throttling миграций.

---

## Трудности в sharding-системах

| Категория      | Проблема                               | Типовое решение                                 |
| -------------- | -------------------------------------- | ----------------------------------------------- |
| Querying       | Cross-shard joins, aggregates          | Scatter-Gather, Query Router, Federated queries |
| Transactions   | Невозможность ACID в нескольких шардах | 2PC, Saga, Outbox pattern                       |
| Backups        | Координированные снапшоты              | Global checkpoint или write-ahead log replay    |
| Monitoring     | Метрики по каждому шару                | Unified metrics + tag per shard                 |
| Schema changes | Разнородные данные на нодах            | Rolling migrations                              |

---

## Anti-patterns

- Использование автоинкрементных ключей при range-based шардировании → hotspot на последнем шарде.
- Ручное управление shard mapping без автоматизации → частые human errors.
- Непрозрачное шардирование на уровне приложения (application-level routing без централизованного координатора).

---

## Примеры реализаций

| Система            | Подход                        | Особенности                    |
| ------------------ | ----------------------------- | ------------------------------ |
| **MongoDB**        | Auto-sharding, range/hash     | Config servers + mongos router |
| **Cassandra**      | Consistent hashing            | Virtual nodes, no master       |
| **Elasticsearch**  | Primary/replica shards        | Automatic balancing            |
| **Vitess (MySQL)** | Range-based, lookup table     | Query routing layer            |
| **Spanner**        | Range-based dynamic splitting | True global consistency        |
